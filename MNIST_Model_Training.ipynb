{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Handwritten digit generation using Conditional Generative Adversarial Networks (Conditional GANs)"
      ],
      "metadata": {
        "id": "J_736aNTb8T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6EWDNLviYwva",
        "outputId": "06ef891c-0183-468d-9123-5a448d0ba62c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB1STQchYl95",
        "outputId": "37e14ed7-69f2-47a2-d252-fa36d2fa211f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.12MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 135kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.28MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.92MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/50] [Batch 0/469] [D loss: 0.7027] [G loss: 0.7059]\n",
            "[Epoch 1/50] [Batch 100/469] [D loss: 0.2328] [G loss: 2.3878]\n",
            "[Epoch 1/50] [Batch 200/469] [D loss: 0.5293] [G loss: 0.5677]\n",
            "[Epoch 1/50] [Batch 300/469] [D loss: 0.5890] [G loss: 0.7018]\n",
            "[Epoch 1/50] [Batch 400/469] [D loss: 0.5416] [G loss: 1.2125]\n",
            "[Epoch 2/50] [Batch 0/469] [D loss: 0.4786] [G loss: 1.0140]\n",
            "[Epoch 2/50] [Batch 100/469] [D loss: 0.4451] [G loss: 1.3697]\n",
            "[Epoch 2/50] [Batch 200/469] [D loss: 0.3464] [G loss: 1.1266]\n",
            "[Epoch 2/50] [Batch 300/469] [D loss: 0.4900] [G loss: 1.0514]\n",
            "[Epoch 2/50] [Batch 400/469] [D loss: 0.4474] [G loss: 0.8960]\n",
            "[Epoch 3/50] [Batch 0/469] [D loss: 0.4363] [G loss: 0.9887]\n",
            "[Epoch 3/50] [Batch 100/469] [D loss: 0.4613] [G loss: 0.9483]\n",
            "[Epoch 3/50] [Batch 200/469] [D loss: 0.5714] [G loss: 0.5471]\n",
            "[Epoch 3/50] [Batch 300/469] [D loss: 0.5680] [G loss: 0.4758]\n",
            "[Epoch 3/50] [Batch 400/469] [D loss: 0.3731] [G loss: 1.1556]\n",
            "[Epoch 4/50] [Batch 0/469] [D loss: 0.3831] [G loss: 1.5514]\n",
            "[Epoch 4/50] [Batch 100/469] [D loss: 0.5650] [G loss: 0.5509]\n",
            "[Epoch 4/50] [Batch 200/469] [D loss: 0.5843] [G loss: 0.4879]\n",
            "[Epoch 4/50] [Batch 300/469] [D loss: 0.3563] [G loss: 1.3415]\n",
            "[Epoch 4/50] [Batch 400/469] [D loss: 0.4159] [G loss: 0.9380]\n",
            "[Epoch 5/50] [Batch 0/469] [D loss: 0.4476] [G loss: 0.8915]\n",
            "[Epoch 5/50] [Batch 100/469] [D loss: 0.3852] [G loss: 1.6091]\n",
            "[Epoch 5/50] [Batch 200/469] [D loss: 0.5279] [G loss: 0.6194]\n",
            "[Epoch 5/50] [Batch 300/469] [D loss: 0.4862] [G loss: 0.6505]\n",
            "[Epoch 5/50] [Batch 400/469] [D loss: 0.5558] [G loss: 0.6166]\n",
            "[Epoch 6/50] [Batch 0/469] [D loss: 0.3261] [G loss: 1.3395]\n",
            "[Epoch 6/50] [Batch 100/469] [D loss: 0.3753] [G loss: 0.9185]\n",
            "[Epoch 6/50] [Batch 200/469] [D loss: 0.5166] [G loss: 0.7032]\n",
            "[Epoch 6/50] [Batch 300/469] [D loss: 0.5259] [G loss: 2.2806]\n",
            "[Epoch 6/50] [Batch 400/469] [D loss: 0.4767] [G loss: 1.6849]\n",
            "[Epoch 7/50] [Batch 0/469] [D loss: 0.5667] [G loss: 0.6066]\n",
            "[Epoch 7/50] [Batch 100/469] [D loss: 0.6084] [G loss: 1.9899]\n",
            "[Epoch 7/50] [Batch 200/469] [D loss: 0.3915] [G loss: 1.4064]\n",
            "[Epoch 7/50] [Batch 300/469] [D loss: 0.5973] [G loss: 2.9085]\n",
            "[Epoch 7/50] [Batch 400/469] [D loss: 0.4778] [G loss: 1.8505]\n",
            "[Epoch 8/50] [Batch 0/469] [D loss: 0.4879] [G loss: 0.6723]\n",
            "[Epoch 8/50] [Batch 100/469] [D loss: 0.4684] [G loss: 1.7079]\n",
            "[Epoch 8/50] [Batch 200/469] [D loss: 0.3590] [G loss: 1.6480]\n",
            "[Epoch 8/50] [Batch 300/469] [D loss: 0.3645] [G loss: 1.4811]\n",
            "[Epoch 8/50] [Batch 400/469] [D loss: 0.3341] [G loss: 1.1389]\n",
            "[Epoch 9/50] [Batch 0/469] [D loss: 0.4564] [G loss: 0.7081]\n",
            "[Epoch 9/50] [Batch 100/469] [D loss: 0.3774] [G loss: 1.3209]\n",
            "[Epoch 9/50] [Batch 200/469] [D loss: 0.7484] [G loss: 3.1383]\n",
            "[Epoch 9/50] [Batch 300/469] [D loss: 0.3894] [G loss: 1.6560]\n",
            "[Epoch 9/50] [Batch 400/469] [D loss: 0.5246] [G loss: 2.5105]\n",
            "[Epoch 10/50] [Batch 0/469] [D loss: 0.4106] [G loss: 0.8523]\n",
            "[Epoch 10/50] [Batch 100/469] [D loss: 0.3183] [G loss: 1.3172]\n",
            "[Epoch 10/50] [Batch 200/469] [D loss: 0.4541] [G loss: 0.7404]\n",
            "[Epoch 10/50] [Batch 300/469] [D loss: 0.2593] [G loss: 2.5259]\n",
            "[Epoch 10/50] [Batch 400/469] [D loss: 0.4463] [G loss: 0.8318]\n",
            "[Epoch 11/50] [Batch 0/469] [D loss: 0.4829] [G loss: 0.6251]\n",
            "[Epoch 11/50] [Batch 100/469] [D loss: 0.2185] [G loss: 2.1636]\n",
            "[Epoch 11/50] [Batch 200/469] [D loss: 0.3790] [G loss: 1.0189]\n",
            "[Epoch 11/50] [Batch 300/469] [D loss: 0.3443] [G loss: 1.1459]\n",
            "[Epoch 11/50] [Batch 400/469] [D loss: 0.3588] [G loss: 1.1717]\n",
            "[Epoch 12/50] [Batch 0/469] [D loss: 0.3689] [G loss: 1.8936]\n",
            "[Epoch 12/50] [Batch 100/469] [D loss: 0.4332] [G loss: 1.6715]\n",
            "[Epoch 12/50] [Batch 200/469] [D loss: 0.4632] [G loss: 2.1947]\n",
            "[Epoch 12/50] [Batch 300/469] [D loss: 0.3707] [G loss: 1.2661]\n",
            "[Epoch 12/50] [Batch 400/469] [D loss: 0.3668] [G loss: 1.4190]\n",
            "[Epoch 13/50] [Batch 0/469] [D loss: 0.3483] [G loss: 2.5484]\n",
            "[Epoch 13/50] [Batch 100/469] [D loss: 0.5049] [G loss: 0.6505]\n",
            "[Epoch 13/50] [Batch 200/469] [D loss: 0.5310] [G loss: 2.8225]\n",
            "[Epoch 13/50] [Batch 300/469] [D loss: 0.3127] [G loss: 1.2862]\n",
            "[Epoch 13/50] [Batch 400/469] [D loss: 0.4622] [G loss: 0.8487]\n",
            "[Epoch 14/50] [Batch 0/469] [D loss: 0.3414] [G loss: 1.0927]\n",
            "[Epoch 14/50] [Batch 100/469] [D loss: 0.4283] [G loss: 0.6914]\n",
            "[Epoch 14/50] [Batch 200/469] [D loss: 0.3316] [G loss: 2.1565]\n",
            "[Epoch 14/50] [Batch 300/469] [D loss: 0.3381] [G loss: 1.4062]\n",
            "[Epoch 14/50] [Batch 400/469] [D loss: 0.3884] [G loss: 0.8261]\n",
            "[Epoch 15/50] [Batch 0/469] [D loss: 0.4659] [G loss: 1.9721]\n",
            "[Epoch 15/50] [Batch 100/469] [D loss: 0.5842] [G loss: 2.8412]\n",
            "[Epoch 15/50] [Batch 200/469] [D loss: 0.3590] [G loss: 2.1723]\n",
            "[Epoch 15/50] [Batch 300/469] [D loss: 0.3347] [G loss: 1.2265]\n",
            "[Epoch 15/50] [Batch 400/469] [D loss: 0.3970] [G loss: 1.0765]\n",
            "[Epoch 16/50] [Batch 0/469] [D loss: 0.3433] [G loss: 1.3807]\n",
            "[Epoch 16/50] [Batch 100/469] [D loss: 0.3118] [G loss: 1.3055]\n",
            "[Epoch 16/50] [Batch 200/469] [D loss: 0.3170] [G loss: 1.1629]\n",
            "[Epoch 16/50] [Batch 300/469] [D loss: 0.4002] [G loss: 1.2820]\n",
            "[Epoch 16/50] [Batch 400/469] [D loss: 0.3360] [G loss: 1.1861]\n",
            "[Epoch 17/50] [Batch 0/469] [D loss: 0.3545] [G loss: 0.9681]\n",
            "[Epoch 17/50] [Batch 100/469] [D loss: 0.2440] [G loss: 2.1190]\n",
            "[Epoch 17/50] [Batch 200/469] [D loss: 0.2984] [G loss: 1.3415]\n",
            "[Epoch 17/50] [Batch 300/469] [D loss: 0.2916] [G loss: 2.7647]\n",
            "[Epoch 17/50] [Batch 400/469] [D loss: 0.3051] [G loss: 1.8549]\n",
            "[Epoch 18/50] [Batch 0/469] [D loss: 0.2323] [G loss: 1.3550]\n",
            "[Epoch 18/50] [Batch 100/469] [D loss: 0.2874] [G loss: 2.3759]\n",
            "[Epoch 18/50] [Batch 200/469] [D loss: 0.2915] [G loss: 1.6792]\n",
            "[Epoch 18/50] [Batch 300/469] [D loss: 0.5354] [G loss: 0.5761]\n",
            "[Epoch 18/50] [Batch 400/469] [D loss: 0.2601] [G loss: 1.4915]\n",
            "[Epoch 19/50] [Batch 0/469] [D loss: 0.1968] [G loss: 2.2376]\n",
            "[Epoch 19/50] [Batch 100/469] [D loss: 0.4149] [G loss: 0.8097]\n",
            "[Epoch 19/50] [Batch 200/469] [D loss: 0.4409] [G loss: 3.1824]\n",
            "[Epoch 19/50] [Batch 300/469] [D loss: 0.2949] [G loss: 1.6240]\n",
            "[Epoch 19/50] [Batch 400/469] [D loss: 0.2388] [G loss: 2.1725]\n",
            "[Epoch 20/50] [Batch 0/469] [D loss: 0.3084] [G loss: 2.0542]\n",
            "[Epoch 20/50] [Batch 100/469] [D loss: 0.3800] [G loss: 0.7883]\n",
            "[Epoch 20/50] [Batch 200/469] [D loss: 0.4562] [G loss: 0.7661]\n",
            "[Epoch 20/50] [Batch 300/469] [D loss: 0.2781] [G loss: 1.2232]\n",
            "[Epoch 20/50] [Batch 400/469] [D loss: 0.2931] [G loss: 1.4405]\n",
            "[Epoch 21/50] [Batch 0/469] [D loss: 0.6164] [G loss: 3.5555]\n",
            "[Epoch 21/50] [Batch 100/469] [D loss: 0.3568] [G loss: 1.1538]\n",
            "[Epoch 21/50] [Batch 200/469] [D loss: 0.4851] [G loss: 0.5937]\n",
            "[Epoch 21/50] [Batch 300/469] [D loss: 0.3106] [G loss: 1.9223]\n",
            "[Epoch 21/50] [Batch 400/469] [D loss: 0.3389] [G loss: 2.4799]\n",
            "[Epoch 22/50] [Batch 0/469] [D loss: 0.3125] [G loss: 1.5744]\n",
            "[Epoch 22/50] [Batch 100/469] [D loss: 0.4378] [G loss: 0.7719]\n",
            "[Epoch 22/50] [Batch 200/469] [D loss: 0.3109] [G loss: 1.1721]\n",
            "[Epoch 22/50] [Batch 300/469] [D loss: 0.2465] [G loss: 2.2173]\n",
            "[Epoch 22/50] [Batch 400/469] [D loss: 0.2908] [G loss: 1.2611]\n",
            "[Epoch 23/50] [Batch 0/469] [D loss: 0.2310] [G loss: 2.0451]\n",
            "[Epoch 23/50] [Batch 100/469] [D loss: 0.1636] [G loss: 1.8093]\n",
            "[Epoch 23/50] [Batch 200/469] [D loss: 0.2785] [G loss: 1.2425]\n",
            "[Epoch 23/50] [Batch 300/469] [D loss: 0.2391] [G loss: 2.0157]\n",
            "[Epoch 23/50] [Batch 400/469] [D loss: 0.3596] [G loss: 1.1180]\n",
            "[Epoch 24/50] [Batch 0/469] [D loss: 0.2669] [G loss: 1.2276]\n",
            "[Epoch 24/50] [Batch 100/469] [D loss: 0.3459] [G loss: 2.1911]\n",
            "[Epoch 24/50] [Batch 200/469] [D loss: 0.3637] [G loss: 1.1043]\n",
            "[Epoch 24/50] [Batch 300/469] [D loss: 0.2283] [G loss: 1.6012]\n",
            "[Epoch 24/50] [Batch 400/469] [D loss: 0.1529] [G loss: 2.7506]\n",
            "[Epoch 25/50] [Batch 0/469] [D loss: 0.3253] [G loss: 1.6553]\n",
            "[Epoch 25/50] [Batch 100/469] [D loss: 0.3120] [G loss: 1.0975]\n",
            "[Epoch 25/50] [Batch 200/469] [D loss: 0.2610] [G loss: 1.4783]\n",
            "[Epoch 25/50] [Batch 300/469] [D loss: 0.2103] [G loss: 1.8334]\n",
            "[Epoch 25/50] [Batch 400/469] [D loss: 0.3585] [G loss: 3.3348]\n",
            "[Epoch 26/50] [Batch 0/469] [D loss: 0.3633] [G loss: 0.9395]\n",
            "[Epoch 26/50] [Batch 100/469] [D loss: 0.2123] [G loss: 2.2700]\n",
            "[Epoch 26/50] [Batch 200/469] [D loss: 0.2092] [G loss: 2.2267]\n",
            "[Epoch 26/50] [Batch 300/469] [D loss: 0.2648] [G loss: 1.7828]\n",
            "[Epoch 26/50] [Batch 400/469] [D loss: 0.3107] [G loss: 1.7645]\n",
            "[Epoch 27/50] [Batch 0/469] [D loss: 0.2587] [G loss: 1.3017]\n",
            "[Epoch 27/50] [Batch 100/469] [D loss: 0.3805] [G loss: 3.6326]\n",
            "[Epoch 27/50] [Batch 200/469] [D loss: 0.2335] [G loss: 1.6827]\n",
            "[Epoch 27/50] [Batch 300/469] [D loss: 0.2735] [G loss: 1.3098]\n",
            "[Epoch 27/50] [Batch 400/469] [D loss: 0.3997] [G loss: 4.1800]\n",
            "[Epoch 28/50] [Batch 0/469] [D loss: 0.6370] [G loss: 0.6874]\n",
            "[Epoch 28/50] [Batch 100/469] [D loss: 0.2666] [G loss: 1.4274]\n",
            "[Epoch 28/50] [Batch 200/469] [D loss: 0.2918] [G loss: 1.2475]\n",
            "[Epoch 28/50] [Batch 300/469] [D loss: 0.2302] [G loss: 2.8041]\n",
            "[Epoch 28/50] [Batch 400/469] [D loss: 0.3041] [G loss: 1.1930]\n",
            "[Epoch 29/50] [Batch 0/469] [D loss: 0.3127] [G loss: 1.8965]\n",
            "[Epoch 29/50] [Batch 100/469] [D loss: 0.2763] [G loss: 1.2781]\n",
            "[Epoch 29/50] [Batch 200/469] [D loss: 0.7661] [G loss: 4.3049]\n",
            "[Epoch 29/50] [Batch 300/469] [D loss: 0.3581] [G loss: 3.0953]\n",
            "[Epoch 29/50] [Batch 400/469] [D loss: 0.6498] [G loss: 4.4869]\n",
            "[Epoch 30/50] [Batch 0/469] [D loss: 0.2223] [G loss: 2.1708]\n",
            "[Epoch 30/50] [Batch 100/469] [D loss: 0.4247] [G loss: 0.8579]\n",
            "[Epoch 30/50] [Batch 200/469] [D loss: 0.6079] [G loss: 0.4293]\n",
            "[Epoch 30/50] [Batch 300/469] [D loss: 0.3005] [G loss: 3.5745]\n",
            "[Epoch 30/50] [Batch 400/469] [D loss: 0.2433] [G loss: 2.6199]\n",
            "[Epoch 31/50] [Batch 0/469] [D loss: 0.3445] [G loss: 1.7117]\n",
            "[Epoch 31/50] [Batch 100/469] [D loss: 0.4029] [G loss: 2.7175]\n",
            "[Epoch 31/50] [Batch 200/469] [D loss: 0.4141] [G loss: 0.8872]\n",
            "[Epoch 31/50] [Batch 300/469] [D loss: 0.3520] [G loss: 3.5734]\n",
            "[Epoch 31/50] [Batch 400/469] [D loss: 0.2052] [G loss: 1.9791]\n",
            "[Epoch 32/50] [Batch 0/469] [D loss: 0.2676] [G loss: 1.2774]\n",
            "[Epoch 32/50] [Batch 100/469] [D loss: 0.1752] [G loss: 2.2518]\n",
            "[Epoch 32/50] [Batch 200/469] [D loss: 0.5003] [G loss: 4.5522]\n",
            "[Epoch 32/50] [Batch 300/469] [D loss: 0.1970] [G loss: 2.2188]\n",
            "[Epoch 32/50] [Batch 400/469] [D loss: 0.4154] [G loss: 4.2822]\n",
            "[Epoch 33/50] [Batch 0/469] [D loss: 0.2502] [G loss: 2.3681]\n",
            "[Epoch 33/50] [Batch 100/469] [D loss: 0.2237] [G loss: 1.7214]\n",
            "[Epoch 33/50] [Batch 200/469] [D loss: 0.1830] [G loss: 2.3415]\n",
            "[Epoch 33/50] [Batch 300/469] [D loss: 0.2714] [G loss: 2.2191]\n",
            "[Epoch 33/50] [Batch 400/469] [D loss: 0.2761] [G loss: 2.1051]\n",
            "[Epoch 34/50] [Batch 0/469] [D loss: 0.3295] [G loss: 1.2974]\n",
            "[Epoch 34/50] [Batch 100/469] [D loss: 0.3229] [G loss: 1.1714]\n",
            "[Epoch 34/50] [Batch 200/469] [D loss: 0.2476] [G loss: 2.0634]\n",
            "[Epoch 34/50] [Batch 300/469] [D loss: 0.3570] [G loss: 0.9913]\n",
            "[Epoch 34/50] [Batch 400/469] [D loss: 0.3691] [G loss: 1.0999]\n",
            "[Epoch 35/50] [Batch 0/469] [D loss: 0.3092] [G loss: 2.0824]\n",
            "[Epoch 35/50] [Batch 100/469] [D loss: 0.3478] [G loss: 3.1677]\n",
            "[Epoch 35/50] [Batch 200/469] [D loss: 0.3482] [G loss: 1.1780]\n",
            "[Epoch 35/50] [Batch 300/469] [D loss: 0.3249] [G loss: 2.8706]\n",
            "[Epoch 35/50] [Batch 400/469] [D loss: 0.2320] [G loss: 1.7613]\n",
            "[Epoch 36/50] [Batch 0/469] [D loss: 0.2319] [G loss: 1.9632]\n",
            "[Epoch 36/50] [Batch 100/469] [D loss: 0.3133] [G loss: 1.4702]\n",
            "[Epoch 36/50] [Batch 200/469] [D loss: 0.2917] [G loss: 2.2735]\n",
            "[Epoch 36/50] [Batch 300/469] [D loss: 0.2672] [G loss: 1.7589]\n",
            "[Epoch 36/50] [Batch 400/469] [D loss: 0.2484] [G loss: 2.3314]\n",
            "[Epoch 37/50] [Batch 0/469] [D loss: 0.2251] [G loss: 1.6844]\n",
            "[Epoch 37/50] [Batch 100/469] [D loss: 0.2594] [G loss: 2.3752]\n",
            "[Epoch 37/50] [Batch 200/469] [D loss: 0.5831] [G loss: 5.4628]\n",
            "[Epoch 37/50] [Batch 300/469] [D loss: 0.2142] [G loss: 1.8018]\n",
            "[Epoch 37/50] [Batch 400/469] [D loss: 0.5413] [G loss: 0.5787]\n",
            "[Epoch 38/50] [Batch 0/469] [D loss: 0.2219] [G loss: 1.4470]\n",
            "[Epoch 38/50] [Batch 100/469] [D loss: 0.1809] [G loss: 1.5654]\n",
            "[Epoch 38/50] [Batch 200/469] [D loss: 0.2785] [G loss: 1.4980]\n",
            "[Epoch 38/50] [Batch 300/469] [D loss: 0.3018] [G loss: 1.2735]\n",
            "[Epoch 38/50] [Batch 400/469] [D loss: 0.2938] [G loss: 1.2035]\n",
            "[Epoch 39/50] [Batch 0/469] [D loss: 0.1922] [G loss: 2.6086]\n",
            "[Epoch 39/50] [Batch 100/469] [D loss: 0.2966] [G loss: 1.2467]\n",
            "[Epoch 39/50] [Batch 200/469] [D loss: 0.2605] [G loss: 1.9662]\n",
            "[Epoch 39/50] [Batch 300/469] [D loss: 0.2660] [G loss: 1.1412]\n",
            "[Epoch 39/50] [Batch 400/469] [D loss: 0.1896] [G loss: 2.0638]\n",
            "[Epoch 40/50] [Batch 0/469] [D loss: 0.3911] [G loss: 2.0331]\n",
            "[Epoch 40/50] [Batch 100/469] [D loss: 0.2742] [G loss: 1.5815]\n",
            "[Epoch 40/50] [Batch 200/469] [D loss: 0.3875] [G loss: 0.9593]\n",
            "[Epoch 40/50] [Batch 300/469] [D loss: 0.1933] [G loss: 1.5114]\n",
            "[Epoch 40/50] [Batch 400/469] [D loss: 0.2596] [G loss: 1.2043]\n",
            "[Epoch 41/50] [Batch 0/469] [D loss: 0.3394] [G loss: 1.2793]\n",
            "[Epoch 41/50] [Batch 100/469] [D loss: 0.2165] [G loss: 1.9406]\n",
            "[Epoch 41/50] [Batch 200/469] [D loss: 0.2255] [G loss: 2.5801]\n",
            "[Epoch 41/50] [Batch 300/469] [D loss: 0.2950] [G loss: 2.8103]\n",
            "[Epoch 41/50] [Batch 400/469] [D loss: 0.3169] [G loss: 3.6301]\n",
            "[Epoch 42/50] [Batch 0/469] [D loss: 0.2708] [G loss: 1.6999]\n",
            "[Epoch 42/50] [Batch 100/469] [D loss: 0.4547] [G loss: 3.9475]\n",
            "[Epoch 42/50] [Batch 200/469] [D loss: 0.1744] [G loss: 2.1542]\n",
            "[Epoch 42/50] [Batch 300/469] [D loss: 0.1854] [G loss: 2.0305]\n",
            "[Epoch 42/50] [Batch 400/469] [D loss: 0.2206] [G loss: 2.1572]\n",
            "[Epoch 43/50] [Batch 0/469] [D loss: 0.1729] [G loss: 2.0821]\n",
            "[Epoch 43/50] [Batch 100/469] [D loss: 0.1831] [G loss: 2.5651]\n",
            "[Epoch 43/50] [Batch 200/469] [D loss: 0.2221] [G loss: 1.9922]\n",
            "[Epoch 43/50] [Batch 300/469] [D loss: 0.2696] [G loss: 2.7603]\n",
            "[Epoch 43/50] [Batch 400/469] [D loss: 0.2306] [G loss: 3.4504]\n",
            "[Epoch 44/50] [Batch 0/469] [D loss: 0.1332] [G loss: 2.0273]\n",
            "[Epoch 44/50] [Batch 100/469] [D loss: 0.2813] [G loss: 2.3933]\n",
            "[Epoch 44/50] [Batch 200/469] [D loss: 0.2297] [G loss: 3.0571]\n",
            "[Epoch 44/50] [Batch 300/469] [D loss: 0.2313] [G loss: 1.7877]\n",
            "[Epoch 44/50] [Batch 400/469] [D loss: 0.1928] [G loss: 2.6887]\n",
            "[Epoch 45/50] [Batch 0/469] [D loss: 0.4706] [G loss: 4.6804]\n",
            "[Epoch 45/50] [Batch 100/469] [D loss: 0.2586] [G loss: 1.9592]\n",
            "[Epoch 45/50] [Batch 200/469] [D loss: 0.3242] [G loss: 2.9183]\n",
            "[Epoch 45/50] [Batch 300/469] [D loss: 0.2175] [G loss: 1.5886]\n",
            "[Epoch 45/50] [Batch 400/469] [D loss: 0.1546] [G loss: 1.7569]\n",
            "[Epoch 46/50] [Batch 0/469] [D loss: 0.1934] [G loss: 1.9667]\n",
            "[Epoch 46/50] [Batch 100/469] [D loss: 0.2877] [G loss: 1.3051]\n",
            "[Epoch 46/50] [Batch 200/469] [D loss: 0.2163] [G loss: 2.8310]\n",
            "[Epoch 46/50] [Batch 300/469] [D loss: 0.2713] [G loss: 1.2599]\n",
            "[Epoch 46/50] [Batch 400/469] [D loss: 0.3206] [G loss: 1.1380]\n",
            "[Epoch 47/50] [Batch 0/469] [D loss: 0.2339] [G loss: 1.6388]\n",
            "[Epoch 47/50] [Batch 100/469] [D loss: 0.2738] [G loss: 2.5478]\n",
            "[Epoch 47/50] [Batch 200/469] [D loss: 0.1787] [G loss: 3.5526]\n",
            "[Epoch 47/50] [Batch 300/469] [D loss: 0.2030] [G loss: 2.6092]\n",
            "[Epoch 47/50] [Batch 400/469] [D loss: 0.2216] [G loss: 2.4941]\n",
            "[Epoch 48/50] [Batch 0/469] [D loss: 0.1892] [G loss: 1.9427]\n",
            "[Epoch 48/50] [Batch 100/469] [D loss: 0.2866] [G loss: 3.2175]\n",
            "[Epoch 48/50] [Batch 200/469] [D loss: 0.1596] [G loss: 2.0410]\n",
            "[Epoch 48/50] [Batch 300/469] [D loss: 0.2532] [G loss: 1.9275]\n",
            "[Epoch 48/50] [Batch 400/469] [D loss: 0.3444] [G loss: 1.4109]\n",
            "[Epoch 49/50] [Batch 0/469] [D loss: 0.3017] [G loss: 2.9473]\n",
            "[Epoch 49/50] [Batch 100/469] [D loss: 0.2401] [G loss: 2.8104]\n",
            "[Epoch 49/50] [Batch 200/469] [D loss: 0.3384] [G loss: 1.1205]\n",
            "[Epoch 49/50] [Batch 300/469] [D loss: 0.3099] [G loss: 1.2929]\n",
            "[Epoch 49/50] [Batch 400/469] [D loss: 0.2493] [G loss: 1.7057]\n",
            "[Epoch 50/50] [Batch 0/469] [D loss: 0.2503] [G loss: 2.2423]\n",
            "[Epoch 50/50] [Batch 100/469] [D loss: 0.1898] [G loss: 1.8884]\n",
            "[Epoch 50/50] [Batch 200/469] [D loss: 0.2423] [G loss: 2.1666]\n",
            "[Epoch 50/50] [Batch 300/469] [D loss: 0.3085] [G loss: 1.6499]\n",
            "[Epoch 50/50] [Batch 400/469] [D loss: 0.1986] [G loss: 2.3455]\n",
            "✅ Generator saved to 'generator.pth'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "z_dim = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==================== Generator ====================\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=100, label_dim=10):\n",
        "        super(Generator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(label_dim, label_dim)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(z_dim + label_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        label_input = self.label_emb(labels)\n",
        "        x = torch.cat([z, label_input], dim=1)\n",
        "        img = self.model(x)\n",
        "        return img.view(-1, 1, 28, 28)\n",
        "\n",
        "# ==================== Discriminator ====================\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, label_dim=10):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(label_dim, label_dim)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784 + label_dim, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        label_input = self.label_emb(labels)\n",
        "        x = torch.cat([img_flat, label_input], dim=1)\n",
        "        validity = self.model(x)\n",
        "        return validity\n",
        "\n",
        "# ==================== Prepare Dataset ====================\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# ==================== Initialize Models ====================\n",
        "generator = Generator(z_dim=z_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# ==================== Training Loop ====================\n",
        "for epoch in range(epochs):\n",
        "    for i, (imgs, labels) in enumerate(dataloader):\n",
        "\n",
        "        batch_size_curr = imgs.size(0)\n",
        "        real_imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Real and fake labels\n",
        "        real = torch.ones(batch_size_curr, 1).to(device)\n",
        "        fake = torch.zeros(batch_size_curr, 1).to(device)\n",
        "\n",
        "        # === Train Generator ===\n",
        "        z = torch.randn(batch_size_curr, z_dim).to(device)\n",
        "        gen_labels = torch.randint(0, 10, (batch_size_curr,)).to(device)\n",
        "        gen_imgs = generator(z, gen_labels)\n",
        "        validity = discriminator(gen_imgs, gen_labels)\n",
        "        g_loss = criterion(validity, real)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # === Train Discriminator ===\n",
        "        real_pred = discriminator(real_imgs, labels)\n",
        "        d_real_loss = criterion(real_pred, real)\n",
        "\n",
        "        fake_pred = discriminator(gen_imgs.detach(), gen_labels)\n",
        "        d_fake_loss = criterion(fake_pred, fake)\n",
        "\n",
        "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Print log\n",
        "        if i % 100 == 0:\n",
        "            print(f\"[Epoch {epoch+1}/{epochs}] [Batch {i}/{len(dataloader)}] \"\n",
        "                  f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
        "\n",
        "# ==================== Save Trained Generator ====================\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "torch.save(generator.state_dict(), \"generator.pth\")\n",
        "print(\"✅ Generator saved to 'generator.pth'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edq5MKzhZb1N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}